name: CI

on:
  push:
    branches:
      - main
      - "feature/**"
      - "feat/**"
      - "bugfix/**"
      - "chore/**"
  pull_request:
    branches:
      - main
  schedule:
    - cron: "0 6 * * *" # daily at 06:00 UTC
  workflow_dispatch:

concurrency:
  group: ci-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit-tests:
    name: Unit tests (3.11) + coverage gates
    runs-on: ubuntu-latest
    permissions:
      contents: read
    env:
      # Keep benches warn-only on PRs; fail on main/nightly
      MERIDIAN_BENCH_WARN_ONLY: "1"
      MERIDIAN_METRICS: "0"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install uv
        shell: bash
        run: |
          python -m pip install --upgrade pip
          pip install uv

      - name: Resolve and sync dependencies
        shell: bash
        run: |
          if [ ! -f uv.lock ]; then
            uv lock
          fi
          uv sync

      - name: Lint (ruff)
        shell: bash
        run: |
          uv run ruff check .

      - name: Format check (black)
        shell: bash
        run: |
          uv run black --check .

      - name: Type check (mypy)
        shell: bash
        run: |
          uv run mypy src

      - name: Run unit tests (pytest)
        shell: bash
        run: |
          uv run pytest -q -k "unit and not benchmark and not stress and not soak" --maxfail=1

      - name: Generate coverage XML (core ≥90%, overall ≥80%)
        shell: bash
        if: always()
        run: |
          set -euo pipefail
          # Overall coverage gate (>=80%)
          uv run pytest --cov=src --cov-report=xml:coverage.xml --cov-fail-under=80 -q -k "unit and not benchmark and not stress and not soak"
          # Optional: core package(s) stricter gate (>=90%) if a 'src/meridian' or similar exists
          if [ -d "src/meridian" ]; then
            uv run pytest --cov=src/meridian --cov-report=term --cov-fail-under=90 -q -k "unit and not benchmark and not stress and not soak"
          fi

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-xml
          path: coverage.xml

  integration-tests:
    name: Integration tests (3.11)
    runs-on: ubuntu-latest
    needs: unit-tests
    permissions:
      contents: read
    env:
      MERIDIAN_METRICS: "1"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install uv
        run: |
          python -m pip install --upgrade pip
          pip install uv

      - name: Resolve and sync dependencies
        run: |
          if [ ! -f uv.lock ]; then
            uv lock
          fi
          uv sync

      - name: Run integration tests
        run: |
          uv run pytest -q -k "integration and not benchmark and not stress and not soak" --maxfail=1

  benchmark-smoke:
    name: Benchmarks (warn-only on PRs)
    runs-on: ubuntu-latest
    needs: unit-tests
    permissions:
      contents: read
    env:
      MERIDIAN_BENCH_WARN_ONLY: "1"
      MERIDIAN_METRICS: "0"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install uv
        run: |
          python -m pip install --upgrade pip
          pip install uv

      - name: Resolve and sync dependencies
        run: |
          if [ ! -f uv.lock ]; then
            uv lock
          fi
          uv sync

      - name: Run microbenchmarks (warn-only on PRs)
        run: |
          set -euo pipefail
          echo "Running minimal microbenchmarks (edge/scheduler)..."
          # Expect benches to be under tests/benchmarks or marked by 'benchmark' keyword
          if uv run pytest -q -k "benchmark" --maxfail=1; then
            echo "Benchmarks passed."
          else
            if [ "${MERIDIAN_BENCH_WARN_ONLY:-0}" = "1" ]; then
              echo "::warning::Benchmarks reported issues; warn-only mode enabled on PRs."
              exit 0
            else
              echo "::error::Benchmarks failed (warn-only disabled)."
              exit 1
            fi
          fi

  docs-snippets:
    name: Validate docs commands
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Detect examples directory
        id: detect_examples
        shell: bash
        run: |
          if [ -d "examples" ] && find examples -type f | grep -q .; then
            echo "run_snippets=true" >> "$GITHUB_OUTPUT"
            echo "Examples detected; will run snippet smoke."
          else
            echo "run_snippets=false" >> "$GITHUB_OUTPUT"
            echo "No examples/ content detected; skipping snippet smoke."
          fi

      - uses: actions/setup-python@v5
        if: ${{ steps.detect_examples.outputs.run_snippets == 'true' }}
        with:
          python-version: "3.11"

      - name: Install uv and tools
        if: ${{ steps.detect_examples.outputs.run_snippets == 'true' }}
        run: |
          python -m pip install --upgrade pip
          pip install uv

      - name: Sync deps
        if: ${{ steps.detect_examples.outputs.run_snippets == 'true' }}
        run: |
          if [ ! -f uv.lock ]; then
            uv lock
          fi
          uv sync

      - name: Run example entrypoints (smoke)
        run: |
          set -euo pipefail
          failed=0

          echo "Running hello_graph via module form..."
          if ! uv run python -m examples.hello_graph.main 2>&1 | tee hello_graph.log; then
            echo "::error::hello_graph failed when running as module (-m). See hello_graph.log for traceback. Check examples/hello_graph/main.py for regressions."
            failed=1
          else
            echo "hello_graph completed successfully."
          fi

          echo "Running pipeline_demo via module form..."
          if ! uv run python -m examples.pipeline_demo.main 2>&1 | tee pipeline_demo.log; then
            echo "::error::pipeline_demo failed when running as module (-m). See pipeline_demo.log for traceback. Check examples/pipeline_demo/main.py for regressions."
            failed=1
          else
            echo "pipeline_demo completed successfully."
          fi

          echo "## Docs Snippet Smoke Summary" >> "$GITHUB_STEP_SUMMARY"
          if [ -f hello_graph.log ]; then
            echo "### hello_graph (last 100 lines)" >> "$GITHUB_STEP_SUMMARY"
            tail -n 100 hello_graph.log >> "$GITHUB_STEP_SUMMARY" || true
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi
          if [ -f pipeline_demo.log ]; then
            echo "### pipeline_demo (last 100 lines)" >> "$GITHUB_STEP_SUMMARY"
            tail -n 100 pipeline_demo.log >> "$GITHUB_STEP_SUMMARY" || true
            echo "" >> "$GITHUB_STEP_SUMMARY"
          fi

          if [ "$failed" -ne 0 ]; then
            echo "::error::One or more example smokes failed. See logs and step summary above."
            exit 1
          fi

          echo "All docs snippet smokes completed successfully."

  nightly-stress-soak:
    name: Nightly stress & soak (short)
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    permissions:
      contents: read
    env:
      MERIDIAN_METRICS: "1"
      MERIDIAN_BENCH_WARN_ONLY: "0"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install uv
        run: |
          python -m pip install --upgrade pip
          pip install uv

      - name: Resolve and sync dependencies
        run: |
          if [ ! -f uv.lock ]; then
            uv lock
          fi
          uv sync

      - name: Run stress tests (short)
        run: |
          # Keep stress under ~5 minutes to be CI-friendly
          UV_PYTEST_ADDOPTS="-q --maxfail=1" uv run pytest -k "stress" || exit 1

      - name: Run soak tests (short)
        run: |
          # Shortened soak for nightly: 10-15 minutes targeted inside tests
          UV_PYTEST_ADDOPTS="-q --maxfail=1" uv run pytest -k "soak" || exit 1

      - name: Run benchmarks (enforced)
        run: |
          set -euo pipefail
          echo "Running benchmarks with enforcement on nightly..."
          MERIDIAN_BENCH_WARN_ONLY=0 uv run pytest -q -k "benchmark" --maxfail=1

  docs-build:
    name: Build MkDocs site
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install MkDocs and plugins
        run: |
          python -m pip install --upgrade pip
          pip install "mkdocs==1.5.3" "mkdocs-material==9.5.17" "mkdocs-git-revision-date-localized-plugin==1.2.4"
      - name: Build
        run: mkdocs build --strict

  link-check:
    name: Check links (lychee)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Prepare lychee cache
        id: lychee-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/lychee
          key: lychee-${{ runner.os }}-${{ hashFiles('.lycheeignore') }}
          restore-keys: |
            lychee-${{ runner.os }}-

      - name: Show lychee config note
        run: |
          echo "Using .lycheeignore for transient domains and known flaky targets"
          echo "Retries enabled via lychee flags to reduce flakiness"
          echo "Caching lychee at ~/.cache/lychee to speed up subsequent runs"

      - name: Run lychee (docs and repo root)
        id: lychee
        uses: lycheeverse/lychee-action@v1.10.0
        with:
          args: >
            --verbose
            --no-progress
            --accept 200,206,429
            --max-redirects 5
            --timeout 25
            --retry-wait-time 3
            --max-retries 3
            --include-fragments
            --exclude-all-private
            --exclude-file .lycheeignore
            --scheme https
            --format markdown
            --output lychee-report.md
            docs
            .
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload lychee report artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: lychee-report
          path: lychee-report.md
          if-no-files-found: warn
          retention-days: 7

      - name: Print lychee summary
        if: always()
        run: |
          echo "## Link Check Summary" >> $GITHUB_STEP_SUMMARY
          if [ -f lychee-report.md ]; then
            bad_count=$(grep -cE '^.*(Error|BROKEN|404|Anchor not found).*' lychee-report.md || true)
            echo "- Broken link count (approx): ${bad_count}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### First 100 lines of report" >> $GITHUB_STEP_SUMMARY
            head -n 100 lychee-report.md >> $GITHUB_STEP_SUMMARY
          else
            echo "- Report file not found (lychee may have failed before writing output)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Fail if lychee reported issues
        if: ${{ steps.lychee.outcome == 'failure' }}
        run: |
          echo "::error::Link-check failed. See lychee output above, the lychee-report artifact, and the summary for details."
          exit 1
